<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-19T01:21:43-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wind and Water</title><subtitle>The fascination of what's difficult...</subtitle><author><name>Douglas Lovell</name></author><entry><title type="html">To the moon</title><link href="http://localhost:4000/2019/06/apollo.html" rel="alternate" type="text/html" title="To the moon" /><published>2019-06-18T00:00:00-03:00</published><updated>2019-06-18T00:00:00-03:00</updated><id>http://localhost:4000/2019/06/apollo</id><content type="html" xml:base="http://localhost:4000/2019/06/apollo.html">This year, July 20, 2019 marks the fiftieth anniversary of the first
moon landing.

![NASA photo of Buzz Aldrin stepping off of the LM ladder](
/assets/images/SecondMan.jpg)

It's hard to believe it was fifty years ago. Maybe half the people
on earth today weren't alive when we did it. Maybe more. I was a little
kid, and remember watching on a little black and white television as
Neil Armstrong stepped off of the ladder.

The photo is a picture that Neil Armstrong made of Buzz Aldrin following
him a little while later. It's a photo of the second man
setting foot on the moon.
These days we would say, &quot;second person,&quot; but I like
&quot;second man&quot; because they were all men and one day there might be a
second woman.

When I was a kid we thought we'd make a permanent human presense on the moon,
moon colonies. I thought it was the first excursion into space and that
we were really going, that it was just the beginning.
We did get a more or less permanent presence in orbit with SkyLab and
now the International Space Station, but never went back to the moon.

Instead, we got the internet.

To relive the days when human beings went to the moon, there's lots of
cool media:

 - [Apollo 11 (2019)](https://www.imdb.com/title/tt8760684/), this
 year's documentary about the first moon landing.
 - [First Man (2018)](https://www.imdb.com/title/tt1213641/), a drama based
 on the life of Neil Armstrong
 - [For All Mankind (1989)](https://www.imdb.com/title/tt0097372/),
 a documentary
 using only NASA footage and voices of the people involved in the missions,
 incuding astronauts and flight controllers.
 - [Apollo 13 (1995)](https://www.imdb.com/title/tt0112384/), the most
 famous hollywood effort, with Tom Hanks as James Lovell
 - [13 minutes to the moon](https://www.bbc.co.uk/programmes/w13xttx2), a
 BBC podcast currently rolling-out to celebrate the anniversary
 - [Apollo and The Moon](https://www.nasa.gov/johnson/HWHAP/apollo-and-the-moon),
 a NASA Johnson Space Center podcast episode

![NASA photo of LM ascent stage approaching CSM with Earth in the background](
/assets/images/LunarRising.jpg)

Of the movies, my favorite is &quot;For All Mankind&quot;, despite the title, because
it has lots of video and audio of the people involved. Of all, it gives the
greatest feel for the times. It has great music from Brian Eno, and footage
of the astronauts playing, just messing around on a moon walk.

The podcast from BBC is a lot of fun.
It's always slow to get started, but once
it gets going it presents many details I've never heard about.
For one thing, it has interviews and descriptions about
mission control, the software, and
the engineering processes around the effort.

NASA Science currently has a
[lovely page about the moon](https://moon.nasa.gov/)
with landing sites marked, information about the Lunar Reconnaissance Orbiter,
a list of the twenty-four men who travelled there and the
twelve who walked on its surface, photos, and
close-up videos of the lunar surface.
There's also the [Apollo missions page](
https://www.nasa.gov/mission_pages/apollo/index.html) which is a portal
to official, authentic, historical photos, video, and audio.

The articles about the Apollo program on Wikipedia are extensive and detailed.

Watching the movies, back in the sixties, they said &quot;Man&quot; and &quot;Mankind&quot; when
they talked about humans. There were some women involved in the effort,
notably [Margaret Hamilton](
https://en.wikipedia.org/wiki/Margaret_Hamilton_(software_engineer))
at MIT, who wrote a lot of the software that landed us.
These days you see things more correctly worded like,
&quot;Twenty-four human beings have travelled
from the Earth to Moon&quot; although in fact they were all men. The vast majority
of the people directly involved were men. It's striking now how dominant
men were in the culture of science and engineering.
It's striking to hear how embedded that was in the language, how we
talked about the people involved. It was always &quot;men.&quot;

The plaque left at tranquility base, on a leg of the landing stage behind
the ladder says,
&quot;Here men first set foot on the moon ... We came in peace for all mankind.&quot;
No-one gave that a second thought.

![NASA photo of astronaut saluting flag with kit](
/assets/images/LunarSalute.jpg)

Some movies I haven't seen and might try to watch:
 - [To the Moon](https://www.imdb.com/title/tt0784725/), an episode of
 Nova that aired in 1999, near the 30th anniversary
 - [When we left the earth](https://www.imdb.com/title/tt1233514/), seven
 TV miniseries episodes from 2008
 - [Moon Shot (1994)](https://www.imdb.com/title/tt0124010/), a three hour
 TV movie aired around the time of the 25th anniversary

And just like the United States, wouldn't you know? We had to send cars
to the moon. In truth, they were quite useful, enabling exploration
as far as about five miles from the lander.
We left three moon buggies there, delivered on the last three missions.
The last time humans were on the moon was early morning December 14, 1972.</content><author><name>Douglas Lovell</name></author><summary type="html">This year, July 20, 2019 marks the fiftieth anniversary of the first moon landing.</summary></entry><entry><title type="html">Goodbye myFltTime</title><link href="http://localhost:4000/2019/02/goodbye-myflttime.html" rel="alternate" type="text/html" title="Goodbye myFltTime" /><published>2019-02-09T00:00:00-03:00</published><updated>2019-02-09T00:00:00-03:00</updated><id>http://localhost:4000/2019/02/goodbye-myflttime</id><content type="html" xml:base="http://localhost:4000/2019/02/goodbye-myflttime.html">In 2011 I started a project for pilot flight time logging and called it,
&quot;myFltTime.com&quot;. The idea was to do something I knew, and try to build a
software as a service (SAAS) product that would bootstrap into a sustainable
business.

![myFltTime.com landing page](/assets/images/myFltTime.png)

Like every entrepreneur starting a business, I knew that I would succeed.
I was optimistic. This is normal and necessary.

It didn't cost me a lot in terms of dollars to set it up and get it running.
It did cost me a lot of time. In terms of sunken costs and not cutting-loose,
the time invested was the greatest impediment. The more time I put into it,
the less I was willing to hear that it wasn't working and best abandoned.

One of the things that I didn't pay attention to was that it is a
crowded market. There were and are a lot of other solutions for this.
I convinced myself that mine would be the best, and that I had a very clean
migration path that made bootstrapping from paper logs or importing from
another solution pretty friction free. I invested time in that.
I did a pretty good job of it.

More saliently, working as a pilot, I met a lot of pilots. Talking to
them about logging their flight time, I kept hearing that they don't.
Most professional working pilots rely on the logging they're already
doing with their employer.
Those that kept their own logs-- private pilots and the rare professional
--were very attached to the things they were using. Pilots aren't
big risk takers. The reliable known is preferable to them. Change isn't
welcome, and I didn't have good compelling reasons to get them to change.
Fewer than fifty pilots even tried it.

In the past few years I didn't do a lot with it. Very little. But kept it
online, paying the hosting fees, because I couldn't bring myself to throw
away all of the work I'd invested in it.
Now I'm at the place where I wouldn't know what to do if by some fluke
of economics a dozen or so pilots start using it. Or worse, thousands.
Time having passed, the lost time and effort that I had invested in it
is less of a memory. Shutting it down, at last, feels like a positive.
I save the hosting fees. Money kept in pocket. A win.</content><author><name>Douglas Lovell</name></author><summary type="html">In 2011 I started a project for pilot flight time logging and called it, “myFltTime.com”. The idea was to do something I knew, and try to build a software as a service (SAAS) product that would bootstrap into a sustainable business.</summary></entry><entry><title type="html">SCCM and Git</title><link href="http://localhost:4000/2019/01/sccm-and-git.html" rel="alternate" type="text/html" title="SCCM and Git" /><published>2019-01-09T00:00:00-03:00</published><updated>2019-01-09T00:00:00-03:00</updated><id>http://localhost:4000/2019/01/sccm-and-git</id><content type="html" xml:base="http://localhost:4000/2019/01/sccm-and-git.html">There are files in our software systems that we desire to have under
source control, but which we never edit. These are source code configuration
management (SCCM) files such as `Gemfile.lock`, `npm.shrinkwrap.json`,
or `db/schema.rb`.

We keep these files under source control (e.g. git), but should we
allow git to merge them when there are independent changes?
If this looks &quot;tl;dr&quot;, jump now to the conclusion.

Three examples in a [Rails](https://rubyonrails.org/) application are
the `Gemfile.lock` for capturing the configuration of Ruby Gems
and `db/schema.rb` or `db/structure.sql` files that capture the schema
for the database.

An example in a [node.js](https://nodejs.org/en/) application is the
`npm.shrinkwrap.json` file that captures the configuration of Node packages.

We don't edit these files because they are generated by the development
platform as artifacts that capture software configuration.
We keep them under source control, in git, because they capture the current
configuration of dependencies, or of database structure, etc. for the
software system.

## Merging configuration files
So what happens when two developers make a change which requires regeneration
of one of these files? To be concrete, suppose two developers individually
update a gem using the `bundler` program, generating independent
versions of `Gemfile.lock`.

Whichever of these developers is second to commit, they will have to first
merge the other's changes. Here is the ten dollar question: do we allow
git to merge the two versions of `Gemfile.lock`?

To answer it, let's consider:
- Would you think of editing this file by hand? Generally not.
- Will the git merge produce a file identical to the one that would result
if one developer had made both changes? Maybe. Maybe not.
- If git produced a merge conflict, would you reconsider editing by
hand to resolve the conflict? Would you be sure of the result?
- Suppose instead we're talking about the database schema. Are you certain
that merge will always produce the schema that would result if one
developer had made both changes?

Thinking about those questions, it seems prudent not to allow git to
merge these files. What we would prefer is that git leave our version
intact, announce that there is a conflict, and not commit the merge
until we have resolved it.

### Preventing automatic merge by git
We can tell git not to merge a file using the `.gitattributes` file.
The `.gitattributes` lives next to the `.git` directory in the root
of our project (and does not exist unless we create it).

The `man gitattributes` command produces a
[long list](https://www.git-scm.com/docs/gitattributes)
of configuration
options we can set for files managed by git. One of them is the &quot;`merge`&quot;
attribute, found under the heading, &quot;Performing a three-way merge&quot;.
Using the setting as follows:
```
Gemfile.lock -merge
```
in `.gitattributes` will unset merge for Gemfile.lock. Thus git will
&quot;Take the version from the current branch as the tentative merge result,
and declare that the merge has conflicts.&quot;

The setting,
```
Gemfile.lock merge=binary
```
in `.gitattributes` will set merge to binary mode for Gemfile.lock. Thus
git will &quot;Keep the version from your branch in the work tree,
but leave the path in the conflicted state for the user to sort out.&quot;

Either method provides the desired result. In fact, they sound the same.
However it's easy to experiment.

With `Gemfile.lock -merge`, on a conflict, we get the following:

```
warning: Cannot merge binary files: Gemfile.lock (HEAD vs. &lt;commit-id&gt;)
Auto-merging Gemfile.lock
CONFLICT (content): Merge conflict in Gemfile.lock
Automatic merge failed; fix conflicts and then commit the result.
```
The `Gemfile.lock` file has the content from the local (our) version.
The `git status` command yields:
```
On branch master
Your branch and 'origin/master' have diverged,
and have 1 and 1 different commits each, respectively.
  (use &quot;git pull&quot; to merge the remote branch into yours)

You have unmerged paths.
  (fix conflicts and run &quot;git commit&quot;)
  (use &quot;git merge --abort&quot; to abort the merge)

Unmerged paths:
  (use &quot;git add &lt;file&gt;...&quot; to mark resolution)

	both modified:   Gemfile.lock

no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
```

With `Gemfile.lock merge=binary`, on a conflict, we get identical outputs
and the file itself is treated identically.

## Resolving the conflict
Resolution of the conflict means running whatever process is in place
for generating these files, in order to produce the generated content
as it should be. The artifacts from which these files are derived are
present in the merged result.

With `Gemfile.lock`, the independent changes to `Gemfile` have been merged.
(If there is a conflict for that file, we resolve it.) Then all we need
do is run bundler and add the resultant file to the merge:
```
bundle install
git add Gemfile.lock
```

The case for an `npm.shrinkwrap.json` file is analogous. The independent
changes to `package.json` have been merged. Running `npm install` will
generate a new shrinkwrap file that we can add to the commit.

With the database schema files managed by Rails, we picked-up the other
developer's new migration in the merge. We run `rails db:migrate` to
not only make the needed changes to local development and test databases,
but also to produce a new `db/schema.rb` or `db/structure.sql` file that
we add to the merge commit.
See [Merging migrations]({% post_url 2018-03-09-merging-structure %})
for full details.

## Conclusion
In order to properly manage generated SCCM files with git, we:
1. use the `.gitattributes` file to tell git not to merge them, ever,
but rather announce that there is a conflict.
2. use our tooling to regenerate the files from the merged assets.
3. add the newly generated files to the merge commit.</content><author><name>Douglas Lovell</name></author><summary type="html">There are files in our software systems that we desire to have under source control, but which we never edit. These are source code configuration management (SCCM) files such as Gemfile.lock, npm.shrinkwrap.json, or db/schema.rb.</summary></entry><entry><title type="html">Keeping minitest Dry</title><link href="http://localhost:4000/2018/08/dry-minitest.html" rel="alternate" type="text/html" title="Keeping minitest Dry" /><published>2018-08-12T00:00:00-03:00</published><updated>2018-08-12T00:00:00-03:00</updated><id>http://localhost:4000/2018/08/dry-minitest</id><content type="html" xml:base="http://localhost:4000/2018/08/dry-minitest.html">Here's a test written with
[minitest](https://github.com/seattlerb/minitest)
as [extended for Rails](#footnote).
It is from the open source Rails project,
[IACCDB](https://github.com/wbreeze/iaccdb).

```
require 'test_helper'

module Admin
  class MemberControllerTest &lt; ActionController::TestCase
    setup do
      @member_list = create_list(:member, 12)
      @before_attrs = @member_list.first.attributes
      @after_attrs = @before_attrs.merge(
        {'family_name' =&gt; Faker::Name.last_name}
      )
    end

    test 'non-admin cannot view index' do
      get :index
      assert_response :unauthorized
    end

    test 'non-admin cannot show member' do
      get :show, params: { id: @member_list.first.id }
      assert_response :unauthorized
    end

    test 'non-admin cannot patch update' do
      patch :update, params: { id: @after_attrs['id'], member: @after_attrs }
      assert_response :unauthorized
    end

    test 'admin can get index' do
      http_auth_login(:admin)
      get :index
      assert_response :success
    end

    test 'admin can get show' do
      http_auth_login(:admin)
      get :show, params: { id: @member_list.first.id }
      assert_response :success
    end

    test 'admin can patch update' do
      http_auth_login(:admin)
      patch :update, params: { id: @after_attrs['id'], member: @after_attrs }
      assert_response :redirect
      member = Member.find(@after_attrs['id'])
      assert_not_nil(member)
      assert_equal(@after_attrs['family_name'], member.family_name)
    end

  end
end
```

The test validates that an unauthenticated user cannot access some
member administration controller methods.
It validates that an authenticated user can access the methods.

A few things about this test file are troublesome:

- Primarily, I don't like the repetiton of `http_auth_login(:admin)`
  at the front of each of the authenticated tests.
- There is some repetition in the test naming: &quot;non-admin cannot&quot;,
  &quot;admin can&quot;
- There are two groups of tests here that aren't in any way grouped.
  Each group calls the same endpoints with different setup and expected
  results.


## thoughtbot/shoulda-context

One solution is to use the
[thoughtbot/shoulda-context](https://github.com/thoughtbot/shoulda-context)
gem.
The shoulda-context gem adds some DSL to minitest for defining contexts of
tests. Now the non-admin tests and the admin
tests each have their own context group. The admin context group has
additional setup that arranges the http basic authentication.

Here is a link to a
[commit with the diff](
https://github.com/wbreeze/iaccdb/commit/f7f8e3c08ca3856ae70545abca097cde195d51cc)
that shows all of the changes. It adds the gem and includes
the DSL additions
in the `test_helper.rb` file.

Here is the new test file. The test implementations themselves did not
change, and are omitted here. The structure of the test file changed with
`context` and `should` DSL methods.
You can find an additional `setup` block within the
`allow admin` context that takes care of authenticating the
admin user for that context.

```
require 'test_helper'

module Admin
  class MemberControllerTest &lt; ActionController::TestCase
    setup do
      @member_list = create_list(:member, 12)
      @before_attrs = @member_list.first.attributes
      @after_attrs = @before_attrs.merge(
        {'family_name' =&gt; Faker::Name.last_name}
      )
    end

    context 'deny non-admin' do
      should 'get index' do
        # ...
      end

      should 'get show' do
        # ...
      end

      should 'patch update' do
        # ...
      end
    end

    context 'allow admin' do
      setup do
        http_auth_login(:admin)
      end

      should 'get index' do
        # ...
      end

      should 'get show' do
        # ...
      end

      should 'patch update' do
        # ...
      end
    end
  end
end
```
This is more satisfying because it:

- Calls out that we're testing the same endpoints with two setups
- Avoids repeating the `http_auth_login` call on each of the authorized tests
- Avoids repeating `deny non-admin` and `allow admin` in the test names

#### Footnote
Rails adds some DSL shortcuts through ActiveSupport::TestCase for defining test
methods with `test`; defining setup and teardown methods with
`setup` and `teardown`.</content><author><name>Douglas Lovell</name></author><summary type="html">Here’s a test written with minitest as extended for Rails. It is from the open source Rails project, IACCDB.</summary></entry><entry><title type="html">Coroutines</title><link href="http://localhost:4000/2018/04/co-routines.html" rel="alternate" type="text/html" title="Coroutines" /><published>2018-04-17T00:00:00-03:00</published><updated>2018-04-17T00:00:00-03:00</updated><id>http://localhost:4000/2018/04/co-routines</id><content type="html" xml:base="http://localhost:4000/2018/04/co-routines.html">Have you ever wondered how a construction crane grows with the building?
Well, I have. Here's how it works.

A portable crane constructs the initial tower and places the head, with
pivot and counter-balanced long arm, on top. Then the crane starts making
the building.

When the building reaches a height too close to the height of the crane,
the crew stabilizes the tower using a brace against the building.
Then they add sections, about four meters long,
to the tower underneath the head.

![Preparing to extend the crane](/assets/images/CraneConnected.jpg)
![Crane extended](/assets/images/CraneExtended.jpg)
![Construction crane](/assets/images/Crane.jpg)

This process of adding sections underneath the head is fascinating,
at least to me. It's a very clever work of engineering.

The base of the tower, when it was first constructed, is fitted
with a sleeve.
The sleeve is roughly double the length of a section.
- The upper part of the sleeve can stably hold the head of the crane.
- The lower part of the sleeve can stably maintain itself attached to
the tower and move the sleeve up and down on the tower.
- The middle part of the sleeve is hollow, open on one side, and fitted
with rails and catwalks.

The crew:

1. moves the sleeve up to the head.
1. braces the tower below the sleeve.
1. raises a section of tower using the crane itself,
and stations the section on the rails of the sleeve.
1. raises another section of tower or a weight to use as adjustable balance
for the head.
1. attaches the head to the sleeve and detaches it from the tower.
There is much banging and fiddling at this stage as they move the
crane to balance the head, take pressure off of the bolts, line up the
bolt holes, and remove or insert the bolts.
1. raises the sleeve, with the attached head, leaving an open space
between the head and the tower.
They do this very, very slowly.
There is no perceptible motion, only over time, a widening gap.
1. slides the new section of tower into place and attaches it.

In the two close-up pictures of the top of the crane, you can see
the sleeve in position before and while raising the head of the crane
above the existing tower to make room for a new tower section.

In the far-away picture, the crew has added one new section to the
tower and is preparing to mount the next section onto the rails on
the sleeve.

## As a metaphor

The crane builds the building, and having built the building,
builds itself. So clever.

This is something like co-routines in software, which trade work
on two parallel aspects of a task to make progress toward completion.

It is also something like bootstrapping a company. You could think of
the crane as money, and the building as the business. With some initial
money you start the business. When the business grows it can fund
itself for more growth.

Seeing the building go up makes me think of incremental development.
A software system isn't
anything like as regular as the floors of a building. As a building,
software would look more like something from Dr. Seuss.
However, in a sense, adding a floor is a sprint. Extending the tower
is a short sprint or spike to enable more sprints.
Filling-out the floors with fire-protection, plumbing, electrical,
HVAC, walls, ceilings, fixtures, and furnishing may all be planned as
units of work or stories that are no doubt sequenced
and scheduled by the builders.

The metaphor is limited:
- A software system ought to be useful very
early in development, like a shed, then a house, then an office, then
a mall... or whatever. The building isn't useful until it's very nearly
complete. (Other than as bracing for the crane that's building it.)
- It's horrible to try to apply construction planning and scheduling
techniques to software. Software isn't regular enough or made-up of
repeated processes- like laying-in sprinkler pipes on the floor of a
building.

In any case,
it's fun to see a construction crew make progress on a building,
to see a crane bootstrap itself, and
to see a development team make progress on a software system.</content><author><name>Douglas Lovell</name></author><summary type="html">Have you ever wondered how a construction crane grows with the building? Well, I have. Here’s how it works.</summary></entry><entry><title type="html">Your Professional Pedigree</title><link href="http://localhost:4000/2018/04/dock-io.html" rel="alternate" type="text/html" title="Your Professional Pedigree" /><published>2018-04-05T00:00:00-03:00</published><updated>2018-04-05T00:00:00-03:00</updated><id>http://localhost:4000/2018/04/dock-io</id><content type="html" xml:base="http://localhost:4000/2018/04/dock-io.html">I was a LinkedIn early adopter in 2007. My profile there was very complete
and well connected. In April, 2017 I got freaked-out about privacy and an
update to LinkedIn terms. (What about them in particular I don't recall.)
I deleted my account.

However I've found that professionally, not having a LinkedIn presence
can be a show stopper.
If nothing else, we look people up there to check pedigree.
A profile on LinkedIn has become part of what we use to establish trust.

By allowing LinkedIn to become a trusted arbiter of pedigree, we have
given them a tremendous amount of power. Enter Dock.io.

## Dock.io
[Dock.io](https://dock.io/) supports a clearing-house for professional
data such as educational background, work history, professional connections,
skills, accomplishments, recommendations, and ratings.
The premise is that you, the person
to whom the data refers, have control over the data. And further, that the
data is in an open format that many third parties can read from and write to,
granted access.

My feeling about this is that it could change the landscape of
professional pedigree just a little bit:
- Now my universities can write a signed diploma into my education history.
- Now my employers can write signed records of start and end date, position
and salary into my employment history.
- Now clients can write ratings and recommendations into my consulting
history.

Now I'm not only who I say I am, but who other people say I am.

Consumers of the data can trust that it is valid, and trust the provenance
of the data because Dock.io puts it in a public, distributed block chain.
(The block chain
rewards the proof of work with units of cryptocurrency that have
some value in trade, as a commodity or security, for fiat currency in an open
market.)

While the block chain is readable by anyone, my encrypted professional
record is available only to someone who has a key that I have provided to
them. And this is cool, because now:

- we have control over the keys
- we can monitor who reads the data
- we can monitor who writes the data

## Neuromancer concerns
As pointed out in the opening,
LinkedIn has already become a necessary part of professional life
(at least in the United States). Dock.io would like to replace it,
with benefit that it offers an open solution.

Supposing Dock.io is successful, all of us will need to have our professional
pedigree in the chain in order to be professionally employed.
Employers will start to require access to write performance data, perhaps.
We're talking about the equivalent of a credit history, credit report, or
credit score.
However it won't be only about getting loans, which can be optional.
It will be about getting work, which can be vital. [Footnote](#footnote)

This bothers me more than a little, but here we have somewhat of an
opportunity to choose our poison.
If LinkedIn accomplished the same, as they are well along the
road to doing, our data is in private hands, and for sale to
unknown, unaccountable third parties. With Dock.io our data is
in a distributed block chain owned by no-one,
that we nominally control access to.

I say nominally because parties will begin to require access, and it
will be more and more necessary to provide it.
The big difference will be that we know who asked.
However once granted, how do we know how the party to whom we granted
the access further shares the data?
That will be up to the policies of the parties to whom we grant access.
It will be up to us to insist on strict no-share policies.
Or rather, you share, but only in my Dock.io record where I see you
sharing, see, and nominally control who views.

Dock.io offers some hope that unlike our credit data, that is always for
sale, our personal professional record can be under our own control.
That we succeed, I think, depends on regulatory policy.
In short, I think we're doomed.

Today I put a limited
[profile back on LinkedIn](https://www.linkedin.com/in/dclovell/)
and started rebuilding a network there.  I'm also available at
[Dock.io](https://dock.io?r=douglaslovell:aaaak2fc).
That's a referral link. Check it out.

And elect people who understand what a sieve the current theater
of privacy has become.

#### Footnote
Credit history does cross the line into vital territory.
Some employers require good credit history for certain job roles.
Many or most landlords require good credit history to rent apartments.</content><author><name>Douglas Lovell</name></author><summary type="html">I was a LinkedIn early adopter in 2007. My profile there was very complete and well connected. In April, 2017 I got freaked-out about privacy and an update to LinkedIn terms. (What about them in particular I don’t recall.) I deleted my account.</summary></entry><entry><title type="html">The Dispossessed</title><link href="http://localhost:4000/2018/03/the-dispossessed.html" rel="alternate" type="text/html" title="The Dispossessed" /><published>2018-03-18T00:00:00-03:00</published><updated>2018-03-18T00:00:00-03:00</updated><id>http://localhost:4000/2018/03/the-dispossessed</id><content type="html" xml:base="http://localhost:4000/2018/03/the-dispossessed.html">When I read about the [death of Ursula K Le Guin](
https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html
) in the New York Times, at the end of January, I thought
how agreeable she sounded, remembered that I hadn't found her books
accessible when I first encountered them as an adolescent,
for whatever reason, and
decided to try reading one of them again.

It isn't strange, but a little regrettable how an artist's stock can
go up when they die. It's only that so many people have that same reaction--
being reminded of something they've missed, finding new or renewed
interest in a body of work that, with one inevitable event, has become
a life's work. The regret is that the artist doesn't get to feel the
appreciation.

Her first five books found resonance with me just now, having content
about strangers, aliens living in foreign lands, because I am. After so
many years in the country of my birth now living far away in what is
on one hand much alike-- an urban economy connected and woven into
global commerce and trade--
and on the other hand completely foreign in language, cultural expectations,
experience, and values.

The fifth book, _The Dispossessed_, felt to me like a classic work
on a par with the best of the classics.
I feel amazed that it isn't better known
and more talked about. I feel amazed that it isn't part of the cultural
curriculum that we consider foundational for the well educated.
Or perhaps it is, and I haven't been, and I'm just getting around to it.

When it was published in 1974 it was called a utopian novel. It was a book
about an experimental, anarchistic society,
its successes and shortcomings, and its
relationship to the worlds around it.

It reads to me, personally, now, as a book whose major theme is not the
societies themselves or their relationships (of which this book describes many)
but rather the relationship of the individual to society.
Some think of Ayn Rand as owning that territory.
_The Dispossessed_ reminds me of _The Fountainhead_.
I now think that she shares it.
This book, being outside of any time or place in human experience,
beyond the limits of a human city in a plausible time, demonstrates
the relationship of the individual to society
at the intersection of multiple worlds, civilizations, and governments.

For this reason it has lasting relevance and importance above parity with
_1984_ or _Animal Farm_, _Catcher in the Rye_ or _Fahrenheit 451_ -- all
books that were part of the curriculum when I was young. We weren't asked
to consume Ayn Rand or Thomas Moore. Ursula K Le Guin was new then.

The relationship of the individual to society, the tensions around
realization of individual potentials within the surround of their
community, culture, economy, and government are forever, it seems to me,
in flux and in tension.

How can a poor kid who's smart contribute and find traction
in an area that is dominated by those more privileged and perhaps,
as often happens, insular? How does an individual with potential open
up a group who has captured an area of discourse
or commerce and established what they consider to be a norm?
How does a person transcend their sex,
race, economic status, language, education, or whatever attributes we
perceive to constitute an &quot;other&quot;?
How does a person find work that produces income while realizing the
best expression of what they are, or potentially might be?
How does a society support and encourage the very difficult work
of sitting down and actually working, vs. getting by, or getting along,
or fitting in?
How do we help people who, facing their work, the incredible monster that it is,
fall short, fail, fear, as we all do, give up, or fall into addiction
or despair or depression?

These are to me questions of the greatest importance and measure
of the success of a society.  And that's why I so much liked
Ursula K Le Guin's _The Dispossessed_.</content><author><name>Douglas Lovell</name></author><summary type="html">When I read about the death of Ursula K Le Guin in the New York Times, at the end of January, I thought how agreeable she sounded, remembered that I hadn’t found her books accessible when I first encountered them as an adolescent, for whatever reason, and decided to try reading one of them again.</summary></entry><entry><title type="html">Why I’m not on Facebook</title><link href="http://localhost:4000/2018/03/why-i-dont-use-fb.html" rel="alternate" type="text/html" title="Why I'm not on Facebook" /><published>2018-03-16T00:00:00-03:00</published><updated>2018-03-16T00:00:00-03:00</updated><id>http://localhost:4000/2018/03/why-i-dont-use-fb</id><content type="html" xml:base="http://localhost:4000/2018/03/why-i-dont-use-fb.html"># ... or WhatsApp or Instagram ...

I don't use these programs because they leak information about
who your contacts are, whom you talk with, when, and how often,
to invisible, unaccountable third parties.

Who are these third parties?

From the [Facebook privacy policy](https://www.facebook.com/about/privacy),
&quot;We share information we have about you within the family of companies that
are part of Facebook.&quot;

[Facebook lists](https://www.facebook.com/help/111814505650678)
nine companies which comprise the Facebook family:

- WhatsApp: share with Facebook your contacts and how much you interact with them.
- Instagram LLC: share pictures of people and things of interest to you,
your location.
- Moves: share with Facebook an activity diary of your daily life.
- Onavo: share with Facebook the apps you're using
and how much data you're using on them (possibly more).
- Facebook Payments, Inc.: You think this stuff is free? You're the product.
- Oculus: virtual reality goggles.
- Atlas: measure and track purchasing behavior linked to advertising, target
advertising.
- Masquerade: cross web-site tracking technology.
- CrowdTangle: monitor social media.

The first two are probably familiar.
The next two, maybe you're foolish enough to use them.
Ever heard of Atlas, Masquerade and CrowdTangle?
[CrowdTangle](http://www.crowdtangle.com/features)
is especially interesting.
Know what they do? I've added summaries to give you a clue.

Facebook makes a big deal of your &quot;privacy controls.&quot; You can change all
sorts of settings about who &quot;sees&quot; your posts. This is smoke and mirrors.
The fact is that all of the above collect, collate, merge, and sell access
to **everything** you do on Facebook and elsewhere on the internet.
Your activity is available privately or by subscription,
without accountability to you, for proprietary purposes.

Organizations that collect, collate, monitor, and enhance this data
aren't the true darknet, because they operate under a public
legal framework where they nevertheless hide. What could we call them?
Let's call them the &quot;datalords&quot;.

When you assent to Facebook terms, or WhatsApp, or Instagram, or ...
you assent to all of the above.

## How you leak data to the datalords
When I share my phone number into your contact list, because you use
WhatsApp, the fact that we are somehow socially connected goes to
the datalords. The fact that I myself don't use WhatsApp doesn't matter.
There is still a data block for me and it just got connected to yours.

### How metadata works
Here is an example of how metadata works and how you leak it.
I spend a lot of time abroad, so I have phone numbers in two countries.
One day I made the mistake of sharing my entire contact information with
a WhatsApp user (which is just about anybody). Both of my phone numbers
went into their address book.

Up until that point, my phone number in one country and the one in another
were completely separate data blocks in the store of the datalords.
However, when next they sucked-up the contacts of my friend,
the two numbers were related and a connection made between the two,
up to now separate data blocks. The connection says, &quot;these are likely the
same person.&quot; After all, they are! And now the datalords can connect every
bit of metadata about one with the other. They've enhanced their whole
world picture about who's who connected with whom.

And the metadata doesn't end there. My phone number is associated with my
credit card numbers. Whenever I make a purchase using the credit card,
the purchase data gets tagged on. That includes the location of the purchase,
amount, company, service, sometimes itemized products purchased.

By matching data items across data blocks -- phone number to phone number,
credit card number to credit card number -- all of the additional data items
in those blocks can be associated with a single identity and form an ever
growing and more detailed picture of that identity.

## Why this makes me uneasy
As a moral, law abiding citizen who works hard,
pays his taxes and stays out of trouble,
why should I care?

In early 2017 I wrote a [response](http://wbreeze.com/bio/TOSFB.html)
to an Electronic Frontier Foundation (EFF) call to technology companies,
asking them to shore-up their security practices.
(EFF had bought a full page advertisement in the January,
2017 issue of Wired magazine.)

The response talks about the data mine, machine learning, and records
of government abuses targeted against citizens. It asserts,

&quot;The greatest danger of these databases is that we are only one rogue agent,
or rogue agency, or rogue government away from having our data turned against
us. In a &quot;national security emergency,&quot; or under some other cover, our
government could claim broad, sweeping access to social network, advertising,
and purchase behavior databases for the purpose of identifying and prosecuting
citizens they deem threatening.&quot;

That's why. Because, while ideas can't be squashed,
individuals who hold them can be.

## Alternatives
There aren't really any good, secure alternatives to Facebook that I know of.
I'd like to know. But really, I don't find it necessary to share a selfie
in front of the leaning tower of Pisa with everyone I know or have been
acquainted with.

For messaging, including phone calls and picture sharing with individuals
whom I'm truly close to, I use [Signal messenger](https://www.signal.org/).
Signal is an angel funded small team that continually improves the
function of the app. The application provides one of the few easy, practical
ways to stay in touch without leaking information to the datalords.
I like it.</content><author><name>Douglas Lovell</name></author><summary type="html">… or WhatsApp or Instagram …</summary></entry><entry><title type="html">Merging migrations</title><link href="http://localhost:4000/2018/03/merging-structure.html" rel="alternate" type="text/html" title="Merging migrations" /><published>2018-03-09T00:00:00-03:00</published><updated>2018-03-09T00:00:00-03:00</updated><id>http://localhost:4000/2018/03/merging-structure</id><content type="html" xml:base="http://localhost:4000/2018/03/merging-structure.html">In Rails, the file `db/structure.sql` or `db/schema.rb` (depending)
captures a snapshot of the state of the development database after a
migration, when you run the `db:migrate` or `db:schema:dump` tasks.

What do you do when you have migrations in two different branches
of your development tree? Do you get a merge conflict? How do you resolve it?
What if you don't get a merge conflict?

I'll use the term &quot;schema&quot; now to refer to the file
(`db/structure.sql` or `db/schema.rb`).
It is the file that captures the structure of the database.
Because the schema is generated by a program, not hand written, the best
effort of `git` to try to merge two changes to it will often produce a
result different than what you will get from running the `db:schema:dump`.

Why is that important?

- It might have duplications or content out of sequence.
- The next migration will produce a schema with differences
unrelated to the migration.
- The schema is the only source of truth about the database structure.
It deserves extra care, to keep it aligned with what the framework
and tooling expect and produce.

Here's a process that allows the tooling to keep the schema in order,
combining independent migrations without errors induced by merging.

First, do not allow git to merge the file. Set-up git to always treat
independent changes to the schema as a conflict. You do this by placing
a `.gitattributes` file in the `db` directory with content,
```
schema.rb -merge
structure.sql -merge
```
(Or add those lines to the existing `.gitattributes` file if there is one.)
The reference for doing that is deep down in the documentation of
[gitattributes](https://www.git-scm.com/docs/gitattributes).

Second, when you do get a conflict, resolve it as follows:

If your database migrations come *after* those you picked-up in the merge:
- Abort the merge `git merge --abort`
- Roll back your migrations, however many, `rails db:rollback`
- Pull the changes a second time

In either case,
- Run the migrations picked-up in the merge, `rails db:migrate`
- Accept the newly generated schema, e.g. `git add db/structure.sql`
- Make that merge commit, `git commit`

In this way, your database schema will always be wholly in agreement with
the text wanted by the framework, because it is always and only ever generated
by the framework.</content><author><name>Douglas Lovell</name></author><summary type="html">In Rails, the file db/structure.sql or db/schema.rb (depending) captures a snapshot of the state of the development database after a migration, when you run the db:migrate or db:schema:dump tasks.</summary></entry><entry><title type="html">Git merge v. rebase</title><link href="http://localhost:4000/2018/03/rebase-v-merge.html" rel="alternate" type="text/html" title="Git merge v. rebase" /><published>2018-03-08T00:00:00-03:00</published><updated>2018-03-08T00:00:00-03:00</updated><id>http://localhost:4000/2018/03/rebase-v-merge</id><content type="html" xml:base="http://localhost:4000/2018/03/rebase-v-merge.html">I’ve been looking into the git merge v. rebase endless theological debate.
It comes up over and over. But I’ve only ever gotten into trouble
trying to rebase. Not getting all wrapped up over how my
commit history looks and just merging has given me fewer issues.

I’m seeing a camp that recommends rebasing your local feature branch when
master gets ahead of it, but merging it when you bring it back into master.
Another person says, yeah, do that, but if your rebase has conflicts,
give it up and merge instead.

I’ll paste some links now. …

- [SO: Conflict resolution during rebase](
  https://stackoverflow.com/a/11219380/608359)
- [Atlassian article](
https://www.atlassian.com/git/articles/git-team-workflows-merge-or-rebase)
  has pros and cons and what Atlassian does.
- [SO: When to rebase](https://stackoverflow.com/a/36587353/608359)
- [Atlassian tutorial](
https://www.atlassian.com/git/tutorials/merging-vs-rebasing)
Contains &quot;The Golden Rule of Rebasing&quot; and after two thousand words says,
&quot;that’s all you really need to know to start&quot;.

The gist I’m taking away is that it’s okay (but entirely optional) to rebase
your feature branch occasionally before finally **merging** it back to master.

All of the caveats and warnings I’m seeing in these threads are about rebase.
That tells me something. My friend, KISS says, &quot;Just merge and stop fussing.&quot;</content><author><name>Douglas Lovell</name></author><summary type="html">I’ve been looking into the git merge v. rebase endless theological debate. It comes up over and over. But I’ve only ever gotten into trouble trying to rebase. Not getting all wrapped up over how my commit history looks and just merging has given me fewer issues.</summary></entry></feed>